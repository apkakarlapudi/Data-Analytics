{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter your name(s) here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rohan Chaudhry rc43755 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Naive Bayes and KNN using scikit-learn\n",
    "\n",
    "In this assignment you'll implement the Naive Bayes and KNN classifier to classify patients as either having or not having diabetic retinopathy. For this task we'll be using the same Diabetic Retinopathy data set which was used in the previous assignment on decision trees. You can find additional details about the dataset [here](http://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set).\n",
    "\n",
    "You'll explore how to train Naive Bayes and KNN classifiers using the `scikit-learn` library. The scikit-learn documentation can be found [here](http://scikit-learn.org/stable/documentation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You may add additional import if you want\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import naive_bayes\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1151, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>prescreen</th>\n",
       "      <th>ma2</th>\n",
       "      <th>ma3</th>\n",
       "      <th>ma4</th>\n",
       "      <th>ma5</th>\n",
       "      <th>ma6</th>\n",
       "      <th>ma7</th>\n",
       "      <th>exudate8</th>\n",
       "      <th>exudate9</th>\n",
       "      <th>exudate10</th>\n",
       "      <th>exudate11</th>\n",
       "      <th>exudate12</th>\n",
       "      <th>exudate13</th>\n",
       "      <th>exudate14</th>\n",
       "      <th>exudate15</th>\n",
       "      <th>euDist</th>\n",
       "      <th>diameter</th>\n",
       "      <th>amfm_class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>49.895756</td>\n",
       "      <td>17.775994</td>\n",
       "      <td>5.270920</td>\n",
       "      <td>0.771761</td>\n",
       "      <td>0.018632</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.486903</td>\n",
       "      <td>0.100025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>57.709936</td>\n",
       "      <td>23.799994</td>\n",
       "      <td>3.325423</td>\n",
       "      <td>0.234185</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.520908</td>\n",
       "      <td>0.144414</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>55.831441</td>\n",
       "      <td>27.993933</td>\n",
       "      <td>12.687485</td>\n",
       "      <td>4.852282</td>\n",
       "      <td>1.393889</td>\n",
       "      <td>0.373252</td>\n",
       "      <td>0.041817</td>\n",
       "      <td>0.007744</td>\n",
       "      <td>0.530904</td>\n",
       "      <td>0.128548</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>40.467228</td>\n",
       "      <td>18.445954</td>\n",
       "      <td>9.118901</td>\n",
       "      <td>3.079428</td>\n",
       "      <td>0.840261</td>\n",
       "      <td>0.272434</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.483284</td>\n",
       "      <td>0.114790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>18.026254</td>\n",
       "      <td>8.570709</td>\n",
       "      <td>0.410381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475935</td>\n",
       "      <td>0.123572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>28.356400</td>\n",
       "      <td>6.935636</td>\n",
       "      <td>2.305771</td>\n",
       "      <td>0.323724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502831</td>\n",
       "      <td>0.126741</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>15.448398</td>\n",
       "      <td>9.113819</td>\n",
       "      <td>1.633493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541743</td>\n",
       "      <td>0.139575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20.679649</td>\n",
       "      <td>9.497786</td>\n",
       "      <td>1.223660</td>\n",
       "      <td>0.150382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576318</td>\n",
       "      <td>0.071071</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>66.691933</td>\n",
       "      <td>23.545543</td>\n",
       "      <td>6.151117</td>\n",
       "      <td>0.496372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500073</td>\n",
       "      <td>0.116793</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>71</td>\n",
       "      <td>64</td>\n",
       "      <td>47</td>\n",
       "      <td>22.141784</td>\n",
       "      <td>10.054384</td>\n",
       "      <td>0.874633</td>\n",
       "      <td>0.099780</td>\n",
       "      <td>0.023386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560959</td>\n",
       "      <td>0.109134</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quality  prescreen  ma2  ma3  ma4  ma5  ma6  ma7   exudate8   exudate9  \\\n",
       "0        1          1   22   22   22   19   18   14  49.895756  17.775994   \n",
       "1        1          1   24   24   22   18   16   13  57.709936  23.799994   \n",
       "2        1          1   62   60   59   54   47   33  55.831441  27.993933   \n",
       "3        1          1   55   53   53   50   43   31  40.467228  18.445954   \n",
       "4        1          1   44   44   44   41   39   27  18.026254   8.570709   \n",
       "5        1          1   44   43   41   41   37   29  28.356400   6.935636   \n",
       "6        1          0   29   29   29   27   25   16  15.448398   9.113819   \n",
       "7        1          1    6    6    6    6    2    1  20.679649   9.497786   \n",
       "8        1          1   22   21   18   15   13   10  66.691933  23.545543   \n",
       "9        1          1   79   75   73   71   64   47  22.141784  10.054384   \n",
       "\n",
       "   exudate10  exudate11  exudate12  exudate13  exudate14  exudate15    euDist  \\\n",
       "0   5.270920   0.771761   0.018632   0.006864   0.003923   0.003923  0.486903   \n",
       "1   3.325423   0.234185   0.003903   0.003903   0.003903   0.003903  0.520908   \n",
       "2  12.687485   4.852282   1.393889   0.373252   0.041817   0.007744  0.530904   \n",
       "3   9.118901   3.079428   0.840261   0.272434   0.007653   0.001531  0.483284   \n",
       "4   0.410381   0.000000   0.000000   0.000000   0.000000   0.000000  0.475935   \n",
       "5   2.305771   0.323724   0.000000   0.000000   0.000000   0.000000  0.502831   \n",
       "6   1.633493   0.000000   0.000000   0.000000   0.000000   0.000000  0.541743   \n",
       "7   1.223660   0.150382   0.000000   0.000000   0.000000   0.000000  0.576318   \n",
       "8   6.151117   0.496372   0.000000   0.000000   0.000000   0.000000  0.500073   \n",
       "9   0.874633   0.099780   0.023386   0.000000   0.000000   0.000000  0.560959   \n",
       "\n",
       "   diameter  amfm_class  label  \n",
       "0  0.100025           1      0  \n",
       "1  0.144414           0      0  \n",
       "2  0.128548           0      1  \n",
       "3  0.114790           0      0  \n",
       "4  0.123572           0      1  \n",
       "5  0.126741           0      1  \n",
       "6  0.139575           0      1  \n",
       "7  0.071071           1      0  \n",
       "8  0.116793           0      1  \n",
       "9  0.109134           0      1  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data from csv file\n",
    "col_names = []\n",
    "for i in range(20):\n",
    "    if i == 0:\n",
    "        col_names.append('quality')\n",
    "    if i == 1:\n",
    "        col_names.append('prescreen')\n",
    "    if i >= 2 and i <= 7:\n",
    "        col_names.append('ma' + str(i))\n",
    "    if i >= 8 and i <= 15:\n",
    "        col_names.append('exudate' + str(i))\n",
    "    if i == 16:\n",
    "        col_names.append('euDist')\n",
    "    if i == 17:\n",
    "        col_names.append('diameter')\n",
    "    if i == 18:\n",
    "        col_names.append('amfm_class')\n",
    "    if i == 19:\n",
    "        col_names.append('label')\n",
    "\n",
    "data = pd.read_csv(\"messidor_features.txt\", names = col_names)\n",
    "print(data.shape)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Naive Bayes Classifier\n",
    "Naive Bayes (NB) classifier is a simple probabilistic classifier that is based on applying the Bayes' theorem and assumes a strong (naive) independence between features.\n",
    "\n",
    "`sklearn.naive_bayes.GaussianNB` implements the Gaussian Naive Bayes algorithm for classification. This means that the liklihood of continuous features is estimated using a Gaussian distribution. (Refer to slide 13 of the Naive Bayes powerpoint notes.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Create a `sklearn.naive_bayes.GaussianNB` classifier. Use `sklearn.model_selection.cross_val_score` to do a 10-fold cross validation on the classifier. Display the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.77286356821588\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "data_X = data.drop(['label'],axis=1) # features \n",
    "data_Y = data['label'] # labels \n",
    "\n",
    "\n",
    "clf = sk.naive_bayes.GaussianNB()\n",
    "clf = clf.fit(data_X, data_Y) \n",
    "\n",
    "# Run a 10-fold cross validation\n",
    "scores = cross_val_score(clf, data_X, data_Y, cv=10) \n",
    "# display accuracy \n",
    "print(\"Accuracy:\", scores.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Show the confusion matrix, precision, recall, and F1 score of your classifier.\n",
    "* `cross_val_score` returns the scores of every test fold. There is another function called `cross_val_predict` that returns predicted y values for every record in the test fold. In other words, for each element in the input, `cross_val_predict` returns the prediction that was obtained for that element when it was in the test set. Use `cross_val_predict` and `sklearn.metrics.confusion_matrix` to print the confusion matrix for the classifier.\n",
    "\n",
    "* `sckit-learn` also provides a useful function `sklearn.metrics.classification_report` for evaluating the classifier on a per-class basis. It is a text summary of the precision, recall, and F1 score for each class (support is just the actual class count). Display the classification report for your Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[502  38]\n",
      " [425 186]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.93      0.68       540\n",
      "           1       0.83      0.30      0.45       611\n",
      "\n",
      "   micro avg       0.60      0.60      0.60      1151\n",
      "   macro avg       0.69      0.62      0.56      1151\n",
      "weighted avg       0.69      0.60      0.56      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Use CVPredict to find Y_pred, use matrix to compare the true and predicted Y \n",
    "pred_Y = cross_val_predict(clf, data_X, data_Y, cv=10)\n",
    "i_am_confused = confusion_matrix(data_Y, pred_Y)\n",
    "print('Confusion Matrix:\\n',i_am_confused,'\\n')\n",
    "\n",
    "\n",
    "# Classification Report \n",
    "print(classification_report(data_Y, pred_Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Receiver Operating Characteristic (ROC) curves are a nice way to see how any predictive model can distinguish between the true positives and negatives. It is a plot of the true positive rate against the false positive rate for the different possible thresholds of a binary classifier.\n",
    "\n",
    "- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity)\n",
    "- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the classifier\n",
    "- The area under the curve is a measure of the classifier accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `sklearn.metrics.roc_curve` plot a ROC curve for the Naive Bayes classifier. Also calculate the area under the curve (AUC) using `sklearn.metrics.roc_auc_score`.\n",
    "\n",
    "* We will just do this on a single holdout test set (because it gets more complicated to put this inside of a cross-validation). So, split your data into trainng and test sets using `sklearn.model_selection.train_test_split`. Do an 80/20 split.\n",
    "* Fit the Naive Bayes classifier to the training data by calling the `fit` method on the trainng data.\n",
    "* Now call the `predict_proba` method on your classifier and pass in the test data. This will return a 2D numpy array with one row for each datapoint in the test set and 2 columns. Column index 0 is the probability that this datapoint is in class 0, and column index 1 is the probability that this datapoint is in class 1.\n",
    "* We are going to say that class 1 (having the disease) is the rare/positive class. To create a ROC curve, pass the actual Y labels and the probabilites of class 1 (column index 1 out of your predict_proba result) into `sklearn.metrics.roc_curve`\n",
    "* Pass the fpr and tpr that `roc_curve` returns into the plotting code that we have provided you.\n",
    "* Print the AUC (area under the curve) by using `sklearn.metrics.roc_auc_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.84346868e-001 1.56531320e-002]\n",
      " [3.66181485e-001 6.33818515e-001]\n",
      " [9.96040121e-001 3.95987902e-003]\n",
      " [9.24211379e-001 7.57886205e-002]\n",
      " [9.64266041e-001 3.57339588e-002]\n",
      " [9.99682946e-001 3.17054052e-004]\n",
      " [9.99892953e-001 1.07047308e-004]\n",
      " [9.98705189e-001 1.29481149e-003]\n",
      " [9.95944452e-001 4.05554821e-003]\n",
      " [5.60172980e-007 9.99999440e-001]\n",
      " [9.99911420e-001 8.85797124e-005]\n",
      " [9.99916680e-001 8.33204886e-005]\n",
      " [2.32641500e-088 1.00000000e+000]\n",
      " [9.98458275e-001 1.54172455e-003]\n",
      " [5.15301406e-001 4.84698594e-001]\n",
      " [8.29606975e-001 1.70393025e-001]\n",
      " [9.99892913e-001 1.07087437e-004]\n",
      " [9.93124957e-001 6.87504349e-003]\n",
      " [9.99824823e-001 1.75177038e-004]\n",
      " [9.99924470e-001 7.55295346e-005]\n",
      " [9.93672651e-001 6.32734877e-003]\n",
      " [9.99719929e-001 2.80071121e-004]\n",
      " [9.88447976e-001 1.15520243e-002]\n",
      " [8.53383254e-001 1.46616746e-001]\n",
      " [1.19456030e-083 1.00000000e+000]\n",
      " [3.23453535e-004 9.99676546e-001]\n",
      " [7.99777025e-016 1.00000000e+000]\n",
      " [9.99932564e-001 6.74360575e-005]\n",
      " [9.97129004e-001 2.87099609e-003]\n",
      " [7.21346885e-001 2.78653115e-001]\n",
      " [9.99922877e-001 7.71230070e-005]\n",
      " [9.99867377e-001 1.32622600e-004]\n",
      " [3.29540050e-001 6.70459950e-001]\n",
      " [9.99886621e-001 1.13379327e-004]\n",
      " [9.99943604e-001 5.63964505e-005]\n",
      " [9.87955288e-001 1.20447118e-002]\n",
      " [9.99928119e-001 7.18812407e-005]\n",
      " [7.12492313e-005 9.99928751e-001]\n",
      " [0.00000000e+000 1.00000000e+000]\n",
      " [9.99892374e-001 1.07625718e-004]\n",
      " [9.90115147e-001 9.88485278e-003]\n",
      " [1.08238624e-267 1.00000000e+000]\n",
      " [8.85153570e-001 1.14846430e-001]\n",
      " [9.91142870e-001 8.85712966e-003]\n",
      " [9.99900514e-001 9.94857985e-005]\n",
      " [5.02563534e-001 4.97436466e-001]\n",
      " [9.03498616e-001 9.65013842e-002]\n",
      " [9.99898563e-001 1.01436720e-004]\n",
      " [9.98795150e-001 1.20484981e-003]\n",
      " [1.03544102e-003 9.98964559e-001]\n",
      " [9.99425812e-001 5.74187744e-004]\n",
      " [4.35450868e-003 9.95645491e-001]\n",
      " [7.46716484e-001 2.53283516e-001]\n",
      " [9.99932041e-001 6.79594935e-005]\n",
      " [9.99916857e-001 8.31430913e-005]\n",
      " [2.34399023e-002 9.76560098e-001]\n",
      " [9.94080804e-001 5.91919620e-003]\n",
      " [4.05823547e-001 5.94176453e-001]\n",
      " [3.16174048e-226 1.00000000e+000]\n",
      " [9.99009158e-001 9.90842265e-004]\n",
      " [9.99204161e-001 7.95838877e-004]\n",
      " [9.99932196e-001 6.78041232e-005]\n",
      " [9.99199448e-001 8.00552420e-004]\n",
      " [9.99759250e-001 2.40749589e-004]\n",
      " [9.99789512e-001 2.10488007e-004]\n",
      " [9.99957804e-001 4.21964198e-005]\n",
      " [9.23629986e-001 7.63700140e-002]\n",
      " [9.99940902e-001 5.90977288e-005]\n",
      " [1.87080661e-002 9.81291934e-001]\n",
      " [6.02976764e-156 1.00000000e+000]\n",
      " [9.15387424e-001 8.46125761e-002]\n",
      " [9.90410469e-001 9.58953130e-003]\n",
      " [9.65549676e-001 3.44503241e-002]\n",
      " [9.99321736e-001 6.78264422e-004]\n",
      " [9.89212801e-001 1.07871987e-002]\n",
      " [9.99868457e-001 1.31542972e-004]\n",
      " [9.97626022e-001 2.37397801e-003]\n",
      " [9.41769436e-001 5.82305640e-002]\n",
      " [2.18781352e-031 1.00000000e+000]\n",
      " [8.30290740e-001 1.69709260e-001]\n",
      " [9.95184644e-001 4.81535633e-003]\n",
      " [9.99938678e-001 6.13219900e-005]\n",
      " [5.08551734e-022 1.00000000e+000]\n",
      " [5.50961019e-001 4.49038981e-001]\n",
      " [9.96784825e-001 3.21517496e-003]\n",
      " [9.99591972e-001 4.08028405e-004]\n",
      " [7.25036405e-001 2.74963595e-001]\n",
      " [2.79762169e-009 9.99999997e-001]\n",
      " [9.95727909e-001 4.27209116e-003]\n",
      " [9.99933251e-001 6.67488386e-005]\n",
      " [9.97902318e-001 2.09768211e-003]\n",
      " [9.60621883e-001 3.93781175e-002]\n",
      " [9.99782906e-001 2.17093579e-004]\n",
      " [9.99921022e-001 7.89781990e-005]\n",
      " [3.73401631e-001 6.26598369e-001]\n",
      " [9.99910175e-001 8.98250802e-005]\n",
      " [9.99915607e-001 8.43929521e-005]\n",
      " [9.99872223e-001 1.27776671e-004]\n",
      " [9.99940092e-001 5.99078315e-005]\n",
      " [0.00000000e+000 1.00000000e+000]\n",
      " [9.99888349e-001 1.11650728e-004]\n",
      " [1.50946721e-002 9.84905328e-001]\n",
      " [9.99428731e-001 5.71269194e-004]\n",
      " [9.99947483e-001 5.25166795e-005]\n",
      " [9.98088900e-001 1.91109978e-003]\n",
      " [9.99685161e-001 3.14838818e-004]\n",
      " [9.97104147e-001 2.89585316e-003]\n",
      " [9.43420277e-001 5.65797229e-002]\n",
      " [0.00000000e+000 1.00000000e+000]\n",
      " [9.97424710e-001 2.57528954e-003]\n",
      " [9.96569805e-001 3.43019510e-003]\n",
      " [1.80274227e-006 9.99998197e-001]\n",
      " [9.98935532e-001 1.06446770e-003]\n",
      " [9.97678680e-001 2.32132000e-003]\n",
      " [5.33690496e-001 4.66309504e-001]\n",
      " [9.99488779e-001 5.11220801e-004]\n",
      " [9.39098596e-009 9.99999991e-001]\n",
      " [9.98854790e-001 1.14521040e-003]\n",
      " [9.99874128e-001 1.25872116e-004]\n",
      " [9.97708391e-001 2.29160896e-003]\n",
      " [9.99933300e-001 6.66995764e-005]\n",
      " [8.98108582e-001 1.01891418e-001]\n",
      " [9.96299088e-001 3.70091198e-003]\n",
      " [9.87206181e-001 1.27938192e-002]\n",
      " [9.99704209e-001 2.95790591e-004]\n",
      " [3.93519849e-010 1.00000000e+000]\n",
      " [8.84535008e-001 1.15464992e-001]\n",
      " [9.46426435e-001 5.35735652e-002]\n",
      " [9.99735845e-001 2.64155035e-004]\n",
      " [9.99925139e-001 7.48605310e-005]\n",
      " [9.99834700e-001 1.65300109e-004]\n",
      " [9.97795381e-001 2.20461926e-003]\n",
      " [9.98352578e-001 1.64742222e-003]\n",
      " [9.99913967e-001 8.60329127e-005]\n",
      " [9.98950644e-001 1.04935564e-003]\n",
      " [3.92065371e-016 1.00000000e+000]\n",
      " [9.99820822e-001 1.79177530e-004]\n",
      " [9.99948577e-001 5.14233068e-005]\n",
      " [9.98732560e-001 1.26744004e-003]\n",
      " [9.99922446e-001 7.75541931e-005]\n",
      " [9.98061911e-001 1.93808941e-003]\n",
      " [9.97633734e-001 2.36626606e-003]\n",
      " [1.36655893e-311 1.00000000e+000]\n",
      " [9.76865485e-001 2.31345151e-002]\n",
      " [9.99868615e-001 1.31384938e-004]\n",
      " [9.99814511e-001 1.85489199e-004]\n",
      " [1.94028393e-005 9.99980597e-001]\n",
      " [9.99809459e-001 1.90540908e-004]\n",
      " [9.01335773e-001 9.86642274e-002]\n",
      " [9.99292184e-001 7.07816362e-004]\n",
      " [9.95483203e-001 4.51679712e-003]\n",
      " [9.99935316e-001 6.46839605e-005]\n",
      " [9.97089187e-001 2.91081263e-003]\n",
      " [9.75550072e-001 2.44499277e-002]\n",
      " [9.98111032e-001 1.88896772e-003]\n",
      " [9.99922119e-001 7.78810946e-005]\n",
      " [6.55323162e-001 3.44676838e-001]\n",
      " [1.17232601e-026 1.00000000e+000]\n",
      " [1.00000000e+000 0.00000000e+000]\n",
      " [9.99233951e-001 7.66049038e-004]\n",
      " [9.97631171e-001 2.36882933e-003]\n",
      " [1.30220664e-001 8.69779336e-001]\n",
      " [9.93213086e-001 6.78691414e-003]\n",
      " [9.98802710e-001 1.19729007e-003]\n",
      " [9.99182732e-001 8.17268253e-004]\n",
      " [9.34370716e-001 6.56292838e-002]\n",
      " [9.48503854e-001 5.14961461e-002]\n",
      " [9.92304815e-001 7.69518481e-003]\n",
      " [0.00000000e+000 1.00000000e+000]\n",
      " [9.97074605e-001 2.92539520e-003]\n",
      " [4.97187713e-005 9.99950281e-001]\n",
      " [9.80254883e-001 1.97451166e-002]\n",
      " [9.99916241e-001 8.37590558e-005]\n",
      " [9.99891348e-001 1.08652239e-004]\n",
      " [9.97890079e-001 2.10992120e-003]\n",
      " [9.99839485e-001 1.60514759e-004]\n",
      " [9.99933373e-001 6.66266098e-005]\n",
      " [9.99930487e-001 6.95129654e-005]\n",
      " [0.00000000e+000 1.00000000e+000]\n",
      " [9.16801545e-001 8.31984553e-002]\n",
      " [9.99930654e-001 6.93457918e-005]\n",
      " [9.99837119e-001 1.62881406e-004]\n",
      " [9.99927129e-001 7.28705804e-005]\n",
      " [9.98622469e-001 1.37753058e-003]\n",
      " [9.99865082e-001 1.34918486e-004]\n",
      " [9.64198562e-001 3.58014381e-002]\n",
      " [9.99836352e-001 1.63648066e-004]\n",
      " [1.89342701e-119 1.00000000e+000]\n",
      " [8.62177106e-001 1.37822894e-001]\n",
      " [9.48888194e-001 5.11118057e-002]\n",
      " [9.99889509e-001 1.10490974e-004]\n",
      " [9.25447595e-187 1.00000000e+000]\n",
      " [9.99381285e-001 6.18715195e-004]\n",
      " [3.10069979e-005 9.99968993e-001]\n",
      " [9.99354232e-001 6.45767736e-004]\n",
      " [8.93658875e-001 1.06341125e-001]\n",
      " [9.99638756e-001 3.61243515e-004]\n",
      " [9.99800938e-001 1.99061557e-004]\n",
      " [9.99867484e-001 1.32516337e-004]\n",
      " [9.99851503e-001 1.48496557e-004]\n",
      " [9.99834173e-001 1.65827350e-004]\n",
      " [9.99914760e-001 8.52402979e-005]\n",
      " [2.97437110e-053 1.00000000e+000]\n",
      " [9.99702134e-001 2.97865879e-004]\n",
      " [9.99684669e-001 3.15330927e-004]\n",
      " [9.98751400e-001 1.24859998e-003]\n",
      " [9.99946760e-001 5.32398473e-005]\n",
      " [1.06088072e-009 9.99999999e-001]\n",
      " [9.99923213e-001 7.67866340e-005]\n",
      " [4.11095401e-135 1.00000000e+000]\n",
      " [6.81462186e-041 1.00000000e+000]\n",
      " [9.99777048e-001 2.22951609e-004]\n",
      " [9.92703566e-001 7.29643420e-003]\n",
      " [7.43930615e-003 9.92560694e-001]\n",
      " [0.00000000e+000 1.00000000e+000]\n",
      " [9.97919669e-001 2.08033123e-003]\n",
      " [9.97714736e-001 2.28526400e-003]\n",
      " [9.98678335e-001 1.32166549e-003]\n",
      " [7.43809055e-002 9.25619094e-001]\n",
      " [9.99653610e-001 3.46390443e-004]\n",
      " [9.99934345e-001 6.56550500e-005]\n",
      " [9.97446095e-001 2.55390530e-003]\n",
      " [1.43696248e-003 9.98563038e-001]\n",
      " [9.98614855e-001 1.38514457e-003]\n",
      " [4.41024296e-001 5.58975704e-001]\n",
      " [9.19070035e-001 8.09299654e-002]\n",
      " [9.99066546e-001 9.33453932e-004]\n",
      " [8.58406448e-001 1.41593552e-001]\n",
      " [4.41236472e-001 5.58763528e-001]\n",
      " [9.99924780e-001 7.52197314e-005]\n",
      " [1.60100688e-206 1.00000000e+000]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC Curve:  0.6602290076335878\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from pandas import DataFrame \n",
    "X_train, X_test, y_train, y_test = train_test_split(data_X, data_Y, test_size=0.20)\n",
    "\n",
    "\n",
    "# Fix NBC by using fit() on training data \n",
    "clf_train = sk.naive_bayes.GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "# Call predict_proba \n",
    "print(clf.predict_proba(X_test))\n",
    "# data_Y is a series \n",
    "pp = DataFrame.from_records(clf.predict_proba(X_test)) # a dataframe \n",
    "ok = pp[1]# datatype is series \n",
    "\n",
    "\n",
    "# Create ROC curve \n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, ok)\n",
    "\n",
    "\n",
    "#replace these fpr and tpr with the results of your roc_curve\n",
    "fpr, tpr = fpr, tpr \n",
    "\n",
    "# Do not change this code! This plots the ROC curve.\n",
    "# Just replace the fpr and tpr above with the values from your roc_curve\n",
    "plt.plot([0,1],[0,1],'k--') #plot the diagonal line\n",
    "plt.plot(fpr, tpr, label='NB') #plot the ROC curve\n",
    "plt.xlabel('fpr')\n",
    "plt.ylabel('tpr')\n",
    "plt.title('ROC Curve Naive Bayes')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# print AUC \n",
    "print('Area under ROC Curve: ', sk.metrics.roc_auc_score(y_test,ok)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: K Nearest Neighbor (KNN) Classifier\n",
    "\n",
    "The KNN classifier consists of two stages:-\n",
    "- In the training stage, the classifier takes the training data and simply memorizes it (KNN is a lazy learner)\n",
    "- In the test stage, the classifier compares the test data with the training data and simply returns the maximum occuring label of the k nearest data points.\n",
    "\n",
    "The distance calculation method is central to the algorithm, typically Euclidean distance is used but other distance metrics like Manhattan distance can also be used. By default `sklearn.neighbors.KNeighborsClassifier` uses the Euclidean distance as its metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Notice that you did not scale the data prior to runnng Naive Bayes. But it is critical to scale the data before running Nearest Neighbor. Explain why we don't need to scale the data for NB, but do need to for NB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe k-nearest neighbor algorithm relies on majority voting based on \\nclass membership of 'k' nearest samples for a given test point. \\nThe nearness of samples is typically based on Euclidean distance.\\nWithout scaling, all the KNN are aligned in the direction of the x axis, \\nwhich leads to incorrect classifications. \\n\\nAll features are considered independent of Naive Bayes. \\n\""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The k-nearest neighbor algorithm relies on majority voting based on \n",
    "class membership of 'k' nearest samples for a given test point. \n",
    "The nearness of samples is typically based on Euclidean distance.\n",
    "Without scaling, all the KNN are aligned in the direction of the x axis, \n",
    "which leads to incorrect classifications. \n",
    "\n",
    "All features are considered independent of Naive Bayes. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Use `sklearn.preprocessing.MinMaxScaler` to normalize the dataset’s features from [0,1]. Use the normalized dataset moving forward. Note that MinMaxScaler returns a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "norm_features = scaler.fit_transform(data_X) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Q6. Use `sklearn.neighbors.KNeighborsClassifier` and fit the classifier on the normalized training set for `k = 5`. Use a 10-fold CV to display precision, recall and accuracy values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  63.51124437781108\n",
      "Recall:  62.3532522474881\n",
      "Precision:  67.18874900877266\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = {'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score),\n",
    "           'accuracy' : make_scorer(accuracy_score)}\n",
    "cvs = cross_validate(knn,norm_features, data_Y,cv=10, scoring =scoring)\n",
    "\n",
    "\n",
    "print('Accuracy: ',np.mean(cvs['test_accuracy'])*100)\n",
    "print('Recall: ',np.mean(cvs['test_recall'])*100)\n",
    "print('Precision: ',np.mean(cvs['test_precision'])*100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Use `sklearn.model_selection.GridSearchCV` to find the best value of k for this data. Try k values from 1-30. Display the best value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K Value from Test Data:  {'n_neighbors': 23}\n",
      "Best Test Score (Accuracy):  0.6602953953084274\n",
      "\n",
      "Refer to commented lines to check test-score results\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=23, p=2,\n",
      "           weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "# https://www.ritchieng.com/machine-learning-efficiently-search-tuning-param/\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "k_range = list(range(1, 31))\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\n",
    "grid.fit(norm_features,data_Y) # normalized features and original labels \n",
    "\n",
    "#print single best score (k) \n",
    "print('Best K Value from Test Data: ', grid.best_params_)\n",
    "print('Best Test Score (Accuracy): ', grid.best_score_)\n",
    "print('\\nRefer to commented lines to check test-score results')\n",
    "\n",
    "#uncomment below to view all scores \n",
    "print(grid.best_estimator_) # summary of reuslts  \n",
    "#print(grid.cv_results_) # all results \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Display the accuracy, precision, and recall of a KNN classifier using the value of k that you just found. (Note that the values are improved because you're using the optimal k for this data!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  66.02923538230885\n",
      "Recall:  60.56054997355896\n",
      "Precision:  71.32781696578259\n"
     ]
    }
   ],
   "source": [
    "knn2 = KNeighborsClassifier(n_neighbors=23)\n",
    "\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "scoring = {'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score),\n",
    "           'accuracy' : make_scorer(accuracy_score)}\n",
    "cvs2 = cross_validate(knn2,norm_features, data_Y,cv=10,scoring = scoring)\n",
    "\n",
    "print('Accuracy: ',np.mean(cvs2['test_accuracy'])*100)\n",
    "print('Recall: ',np.mean(cvs2['test_recall'])*100)\n",
    "print('Precision: ',np.mean(cvs2['test_precision'])*100)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Now wrap the whole process in another cross-validation to report the final accuarcy of your KNN model. \n",
    "\n",
    "To perform the nested cross-validation that we discussed in class, you'll now need to pass the `GridSearchCV` into a `cross_val_score`. \n",
    "\n",
    "What this does is: the `cross_val_score` splits the data in to train and test sets for the first fold, and it passes the train set into `GridSearchCV`. `GridSearchCV` then splits that set into train and validation sets for k number of folds (the inner CV loop). The hyper-parameters for which the average score over all inner iterations is best, is reported as the `best_params_`, `best_score_`, and `best_estimator_`(best decision tree). This best decision tree is then evaluated with the test set from the `cross_val_score` (the outer CV loop). And this whole thing is repeated for the remaining k folds of the `cross_val_score` (the outer CV loop). \n",
    "\n",
    "That is a lot of explanation for a very complex (but IMPORTANT) process, which can all be performed with a single line of code!\n",
    "\n",
    "Be patient for this one to run. The nested cross-validation loop can take some time. A [*] next to the cell indicates that it is still running.\n",
    "\n",
    "Print the accuracy, precision, and recall of your tuned, cross-validated model. This is the official accuracy, precision, recall that you would report for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6550825369244135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       count       0.61      0.74      0.67       540\n",
      "       label       0.71      0.58      0.64       611\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      1151\n",
      "   macro avg       0.66      0.66      0.65      1151\n",
      "weighted avg       0.67      0.66      0.65      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "cvp = cross_val_predict(grid, norm_features, data_Y, cv=10)\n",
    "print(accuracy_score(data_Y, cvp)) \n",
    "\n",
    "labels = ['count','label']\n",
    "print(classification_report(data_Y, cvp, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. We discussed why dimensionality reduction is critical to KNN because of the curse of dimensionality. So we may want to perform a dimensionality reduction with PCA before running KNN. (Remember that you can also reduce dimensionality by performing feature selection and feature engineering.) \n",
    "\n",
    "An important note about PCA is that is should only be performed on the **training** data, then you transform the test data into the PCA space that was found on the training data. \n",
    "\n",
    "So when you are doing cross-validation, the PCA needs to happen *inside of your CV loop*. This way, it is performed on the training set for the first fold, then the test set is put into that space. On the second fold, it is performed on the trainng set for the second fold, and the test set is put into that space. And so on for the remaining folds. \n",
    "\n",
    "In order to do this with Python, you must create what's called a `Pipeline` and pass that in to the cross validation. This is a very important concept for Data Mining and Machine Learning, so let's practice it here.\n",
    "\n",
    "We have provided some of the necessary code for you, but this code is not complete. You need to finish it by doing the following:\n",
    "* pass the pipeline and the parameters into a `GridSearchCV` with a 5-fold cross validation\n",
    "* call `fit()` on the GridSearchCV and pass in the normalized data (X_values, Y_values)\n",
    "* print out the `best_score_` and `best_params_` from the GridSearchCV\n",
    "\n",
    "This will show you the best number of principal components to keep (number of dimensions) and the best value of k to use (number of neighbors).\n",
    "\n",
    "[Then of coure you'd want to wrap this GridSearchCV in another cross-validation to do a nested cross-validation and get an accuracy estimate. But we'll leave that for another time! :) ]\n",
    "\n",
    "Again, be patient for this one to run. The GridSearchCV can take some time. A [*] next to the cell indicates that it is still running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grib Best Score:  0.6603124411820064\n",
      "Grib Best Params:  {'knn__n_neighbors': 23, 'pca__n_components': 13}\n"
     ]
    }
   ],
   "source": [
    "# Define a pipeline to search for the best combination of PCA truncation and n_neighbors.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#create a PCA\n",
    "pca = PCA()\n",
    "\n",
    "#create a KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#create a pipeline that does a PCA and a KNN\n",
    "pipe = Pipeline(steps=[('pca', pca), ('knn', knn)])\n",
    "\n",
    "#Set up the parameters you want to tune for each of your pipeline steps\n",
    "#Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {\n",
    "    'pca__n_components': list(range(1, 19)), #find how many principal componenet to keep\n",
    "    'knn__n_neighbors': list(range(1, 30)),  #find the best value of k\n",
    "}\n",
    "\n",
    "# your code goes here:\n",
    "# pass the pipeline and the parameters into a GridSearchCV with a 5-fold cross validation\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', iid=False)\n",
    "\n",
    "# call fit() on the GridSearchCV and pass in the normalized data (X_values, Y_values)\n",
    "grid.fit(norm_features,data_Y) # normalized features and original labels \n",
    "\n",
    "# print out the best_score_ and best_params_ from the GridSearchCV\n",
    "print('Grib Best Score: ', grid.best_score_)\n",
    "print('Grib Best Params: ', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
